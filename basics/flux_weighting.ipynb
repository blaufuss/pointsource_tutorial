{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, numpy as np, matplotlib, scipy\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.random import poisson\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import numpy.lib.recfunctions as rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in both the simulation and the data\n",
    "sim = np.load(\"./data/IC86_2012_MC.npy\")\n",
    "data = np.load(\"./data/IC86_2012_exp.npy\")\n",
    "\n",
    "# Show the possible keys available here:\n",
    "print(\"Keys available in simulation:\")\n",
    "print(sorted(sim.dtype.names))\n",
    "print()\n",
    "\n",
    "print(\"Keys available in data:\")\n",
    "print(sorted(data.dtype.names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also load in the \"GoodRunList\" (GRL), a file that tells\n",
    "# us when the detector was taking good data. \n",
    "grl = np.load(\"./data/GRL/IC86_2012_exp.npy\")\n",
    "\n",
    "# Show the keys available in the GRL\n",
    "print(\"Keys available in the GoodRunList:\")\n",
    "print(sorted(grl.dtype.names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************************\n",
    "# We will need the average rate for our analysis.\n",
    "# We can get this by either counting the number of\n",
    "# events in data or the number of events recorded\n",
    "# in the GRL and dividing by the total good livetime.\n",
    "#******************************************************\n",
    "total_events = len(data)\n",
    "total_livetime = np.sum(grl['livetime'])\n",
    "\n",
    "average_rate = total_events / total_livetime\n",
    "print(\"Data has an average rate of {:4.2f} events/day\".format(average_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************************\n",
    "# Define the parameters of our analysis.\n",
    "# We're going to simplify things a little bit to start\n",
    "# and ignore the impact of detector downtime, which\n",
    "# would need to be included in an actual analysis.\n",
    "# \n",
    "# Our first test analysis will look for an excess of \n",
    "# neutrino events in 1000 seconds centered on T=123.4\n",
    "# using events from the entire sky.\n",
    "#******************************************************\n",
    "analysis_time = 56123.4 # days\n",
    "time_window = 1000 # seconds\n",
    "time_window /= (24*3600.) # converted to days, since our rate is in days.\n",
    "\n",
    "# We will be using the data to model the background in\n",
    "# our test analysis. How many background events should\n",
    "# we expect in our analysis?\n",
    "n_expected = average_rate * time_window\n",
    "print(\"We expect an average of {:4.3f} background events in our \"\\\n",
    "      \"{:4.3f} day time window.\".format(n_expected, time_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************************\n",
    "# Today, we'll also be adding a signal flux model\n",
    "# to our analysis. We'll be assuming a power law \n",
    "# spectrum with two free parameters: \n",
    "# f(E) = N * (E/E0)**-gamma\n",
    "# - N: A normalization term with units of 1/(GeV cm^2 sr s)\n",
    "# - E: The true energy for each simulated neutrino event \n",
    "#      in GeV\n",
    "# - E0: Another normalization term setting the energy scale\n",
    "#       for the flux. While this term isn't necessary, it \n",
    "#       can make it easier to work with some of the values. \n",
    "#       A typical value will be 100 TeV.\n",
    "# - gamma: A spectral index. For astrophysical neutrinos,\n",
    "#       this value will generally be around 2. For atmospheric\n",
    "#       neutrinos, we should get values more like 3.7\n",
    "# In order to include this, we need to write a function\n",
    "# to take a dataset and return \"weights\" for each event,\n",
    "# representing the frequency/rate that we expect to\n",
    "# observe each simulated event.\n",
    "#******************************************************\n",
    "def signal_weight(simulation,\n",
    "                  time_window = time_window,\n",
    "                  N=1, gamma=2):\n",
    "    # Get the effective area per event\n",
    "    event_aeff = simulation['ow']\n",
    "    \n",
    "    # Calculate the flux for each event. This is an assumption\n",
    "    # that you must make as part of your analysis choices. It\n",
    "    # may be a simple power law, like here, or a more complex\n",
    "    # case that depends on time, direction, or type of particle.\n",
    "    # You can use anything that's available as a key in the \n",
    "    # simulation file.\n",
    "    #\n",
    "    # Implement a simple power law flux with E0=100 TeV\n",
    "    event_flux = 0\n",
    "    \n",
    "    # Multiply the flux and effective area to get the rate \n",
    "    # of observing each event in Hz\n",
    "    event_rate = event_aeff * event_flux\n",
    "    \n",
    "    # Integrate (Aeff * flux) over any assumed signal time\n",
    "    # distribution to get the expected number of events. Note \n",
    "    # that the time window we picked above is given in days! \n",
    "    # We'll need to convert it to seconds to make this work.\n",
    "    # Implement this integration, assuming uniform emission\n",
    "    # over the full time window\n",
    "    signal_expected = 0\n",
    "\n",
    "    return signal_expected\n",
    "\n",
    "# Run it once to get a feel for how the N and gamma affect\n",
    "# our expectations.\n",
    "n_signal_example = np.sum(signal_weight(sim, N=1, gamma=2))\n",
    "print(\"For the input E^-2 spectrum, we expect an average of {:4.3f} signal \"\\\n",
    "      \"events in our {:4.3f} day time window.\".format(n_signal_example, time_window))\n",
    "\n",
    "# Try different values for N and gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************************\n",
    "# Now that we have defined the parameters for our\n",
    "# analysis, let's pick out the events that we want\n",
    "# to focus on. \n",
    "# Define a function to pick out these events. For\n",
    "# now, we'll only be applying this to data. Eventually,\n",
    "# we'll also want to apply it to simulation as well\n",
    "# once we have an analysis that eg only looks with \n",
    "# 2 degrees of a GRB or blazar position.\n",
    "#******************************************************\n",
    "def select_events(dataset,\n",
    "                  tstart = analysis_time-time_window/2.,\n",
    "                  tend = analysis_time+time_window/2.,\n",
    "                  is_simulation = False):\n",
    "    \n",
    "    # By default, keep all events\n",
    "    events_to_keep = np.ones(len(dataset), dtype=bool)\n",
    "\n",
    "    # Timing is only meaningful in the data files.\n",
    "    # The time in the simulation files is total junk,\n",
    "    # so we only want to check it if we're looking at data\n",
    "    if not is_simulation:\n",
    "        events_to_keep &= (tstart <= dataset['time'])\n",
    "        events_to_keep &= (dataset['time'] < tend)\n",
    "    \n",
    "    return dataset[events_to_keep]\n",
    "\n",
    "selected_data = select_events(data)\n",
    "print(\"Found {} events\".format(len(selected_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************************\n",
    "# We will need to take data and get some \"test statistic\" \n",
    "# out of them. This will usually be a likelihood, but may\n",
    "# include other things if required. If we use a likelihood,\n",
    "# the \"TS\" value will typically be either the LLH or 2*LLH\n",
    "#\n",
    "# We're no longer planning to specify the number of signal\n",
    "# events, but instead the signal flux parameters. How will\n",
    "# we need to modify this function?\n",
    "#******************************************************\n",
    "def get_test_statistic(trial, \n",
    "                       n_background = n_expected,\n",
    "                       n_signal = 0,\n",
    "                       find_best_fit = True,\n",
    "                       ):\n",
    "    \n",
    "    # Define the binned Poisson likelihood\n",
    "    n_observed = len(trial)\n",
    "    n_expected = n_background + n_signal    \n",
    "    \n",
    "    # To better include the constants, we're going to\n",
    "    # use the scipy.stats.poisson distribution code\n",
    "    # directly. \n",
    "    nllh = -1 * stats.poisson.logpmf(n_observed, n_expected)\n",
    "    return nllh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************************\n",
    "# The TS value we get from the above function is just\n",
    "# a number. A number by itself doesn't tell us much\n",
    "# and can't be used for actual physics. Instead, we\n",
    "# need to learn how to interpret that number.\n",
    "# Almost all analyses we do in IceCube include the\n",
    "# concept of a \"trial\" to learn this interpretation\n",
    "#\n",
    "# A trial is one simulated observation. In our case,\n",
    "# our analysis is looking at 1000 second time windows\n",
    "# which may have signal and background events. Define\n",
    "# a function which can produce a trial using simulation.\n",
    "#\n",
    "# We only care about the time window for now, so we \n",
    "# won't be using any directional information yet. \n",
    "# Note that the n_background and n_signal are the\n",
    "# AVERAGE expectations, meaning that they do not have\n",
    "# to be integers. You can have an *expectation* of \n",
    "# 0.5 events, but you cannot *observe* 0.5 events.\n",
    "#\n",
    "# \n",
    "#******************************************************\n",
    "def produce_trial(simulation,\n",
    "                  n_background = n_expected,\n",
    "                  n_signal = 0.0):\n",
    "    \n",
    "    # Define the function here.\n",
    "    number_bg = np.random.poisson(n_background)\n",
    "    number_sig = np.random.poisson(n_signal)\n",
    "    \n",
    "    background_observed = np.random.choice(simulation, number_bg)\n",
    "    signal_observed = np.random.choice(simulation, number_sig)\n",
    "\n",
    "    #return the total number of events\n",
    "    return np.concatenate([background_observed, signal_observed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a single trial and calculate the TS value\n",
    "# in order to test the functions.\n",
    "trial_events = produce_trial(sim, n_signal=0)\n",
    "trial_ts = get_test_statistic(trial_events, n_signal=0)\n",
    "print(trial_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************************\n",
    "# We still don't know what this value actually means: it's\n",
    "# just a number. In order to interpret the value, we need\n",
    "# to know what we would expect from just background or \n",
    "# from signal+background. We do that by producing many\n",
    "# trials and looking at the distribution of TS values.\n",
    "# Write a function to make this simpler\n",
    "#\n",
    "# How do we modify this function to include the signal\n",
    "# flux parameters?\n",
    "#******************************************************\n",
    "def produce_trials(simulation, \n",
    "                   n_trials = 1000,\n",
    "                   n_background = n_expected, \n",
    "                   n_signal_test = 0,\n",
    "                   n_signal_injected = 0\n",
    "                  ):\n",
    "    ns = np.zeros(n_trials, dtype=float)\n",
    "    ts = np.zeros(n_trials, dtype=float)\n",
    "    for i in range(n_trials):\n",
    "        trial_events = produce_trial(simulation, n_background, n_signal_injected)\n",
    "        ts[i] = get_test_statistic(trial_events, n_background, n_signal_test)\n",
    "        \n",
    "    return ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce the TS values for a bunch of choices for signal\n",
    "# Let's first do this for different values of the flux\n",
    "# normalization, N. Pick values that you think will give\n",
    "# a reasonable number of signal events, given our background\n",
    "# expectations.\n",
    "background_ts = produce_trials(sim, n_trials=1000)\n",
    "signal_5_ts = produce_trials(sim, n_trials=1000, n_signal_test=0, n_signal_injected=5)\n",
    "signal_10_ts = produce_trials(sim, n_trials=1000, n_signal_test=0, n_signal_injected=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And plot them all so we can get a feel for how the\n",
    "# test statistic we defined changes as we vary the\n",
    "# signal parameters\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "bins = np.linspace(0, 50, 35)\n",
    "\n",
    "ax.hist(background_ts,\n",
    "        bins = bins,\n",
    "        density = True,\n",
    "        histtype = 'step',\n",
    "        linewidth = 3,\n",
    "        label = r'$n_{inj}$ = 0')\n",
    "\n",
    "ax.hist(signal_5_ts,\n",
    "        bins = bins,\n",
    "        density = True,\n",
    "        histtype = 'step',\n",
    "        linewidth = 3,\n",
    "        label = r'$n_{inj}$ = 5')\n",
    "\n",
    "ax.hist(signal_10_ts,\n",
    "        bins = bins,\n",
    "        density = True,\n",
    "        histtype = 'step',\n",
    "        linewidth = 3,\n",
    "        label = r'$n_{inj}$ = 10')\n",
    "\n",
    "ax.legend(loc='upper left', fontsize=16,)\n",
    "\n",
    "ax.grid(alpha=0.25)\n",
    "ax.set_xlim(bins.min(), bins.max())\n",
    "ax.set_xlabel(\"-LLH\", fontsize=16)\n",
    "ax.set_ylabel(\"Fraction of Trials\", fontsize=16)\n",
    "ax.tick_params(which='both', labelsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************************\n",
    "# We've seen what the distributions looks like. \n",
    "# Let's calculate a \"sensitivity\". The sensitivity is\n",
    "# used to estimate the amount of signal which can be\n",
    "# hidden in the background and answers the following \n",
    "# question:\n",
    "# What average number of signal events is required so\n",
    "# that 90% of the (signal+background) distribution is\n",
    "# above 50% of the (background only) distribution?\n",
    "# \n",
    "# How can we answer this question?\n",
    "# \n",
    "# Start by finding the median of the background distribution\n",
    "# We do this by running \"background trials\": produce 100000\n",
    "# samples of background. We're going to do this in a single\n",
    "# cell so that we don't have to redo it later\n",
    "#******************************************************\n",
    "background_ts = produce_trials(sim, n_trials=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************************\n",
    "# Now pick a signal flux normalization and produce \n",
    "# \"signal trials\" using the (signal+background) rate\n",
    "#******************************************************\n",
    "n_signal = 4.2\n",
    "signal_ts = produce_trials(sim,\n",
    "                           n_trials = 1000,\n",
    "                           n_background = n_expected,\n",
    "                           n_signal_injected = n_signal)\n",
    "\n",
    "# Show these distributions to get a feel for how\n",
    "# the two distributions look.\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bins = np.linspace(0, 20, 26)\n",
    "\n",
    "_ = ax.hist(background_ts,\n",
    "            bins = bins,\n",
    "            density = True,\n",
    "            color = 'r',\n",
    "            alpha = 0.6,\n",
    "            label = 'Background')\n",
    "\n",
    "_ = ax.hist(signal_ts,\n",
    "            bins = bins,\n",
    "            density = True,\n",
    "            color = 'b',\n",
    "            alpha = 0.6,\n",
    "            label = 'Signal+Background')\n",
    "\n",
    "# Let's also directly show the location of the median \n",
    "# of the background distribution\n",
    "background_median = np.percentile(background_ts, 50)\n",
    "ax.axvline(background_median,\n",
    "           linewidth=3,\n",
    "           color = 'k',\n",
    "           alpha = 0.4,\n",
    "           linestyle = 'dashed',\n",
    "           label = \"Background 50%\")\n",
    "\n",
    "# We want 90% of the (signal+background) to be\n",
    "# above the sensitivity, but np.percentile takes\n",
    "# the percentage *below* instead.\n",
    "signal_10pc = np.percentile(signal_ts, 100-90)\n",
    "ax.axvline(signal_10pc,\n",
    "           linewidth=3,\n",
    "           color = 'k',\n",
    "           alpha = 0.4,\n",
    "           linestyle = 'dotted',\n",
    "           label = \"Signal 90%\")\n",
    "\n",
    "\n",
    "ax.legend(fontsize=16)\n",
    "ax.grid(alpha=0.2)\n",
    "ax.set_xlim(bins.min(), bins.max())\n",
    "ax.set_xlabel(\"Test Statistic\", fontsize=16)\n",
    "ax.set_ylabel(\"Fraction of trials\", fontsize=16,)\n",
    "ax.tick_params(which=\"both\", labelsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******************************************************\n",
    "# In addition to the \"sensitivity\", there's also a\n",
    "# \"discovery potential\". The discovery potential answers\n",
    "# this question:\n",
    "#\n",
    "# How much signal is required to claim a discovery at N-sigma?\n",
    "# \n",
    "# From a practical perspect, the process is almost identical \n",
    "# to that for the sensitivity. Here, we want to know what \n",
    "# signal is required for 50% of the (signal+background) distribution\n",
    "# to be above N-sigma of the background distribution.\n",
    "#\n",
    "# Let's do this for a 3-sigma discovery\n",
    "# 3-sigma corresponds to 99.73%\n",
    "#******************************************************\n",
    "n_signal = 6.5\n",
    "signal_ts = produce_trials(sim,\n",
    "                           n_trials = 1000,\n",
    "                           n_background = n_expected,\n",
    "                           n_signal_injected = n_signal)\n",
    "\n",
    "# Show these distributions to get a feel for how\n",
    "# the two distributions look.\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bins = np.linspace(0, 30, 31)\n",
    "\n",
    "_ = ax.hist(background_ts,\n",
    "            bins = bins,\n",
    "            weights = np.ones(len(background_ts), dtype=float)/len(background_ts),\n",
    "            color = 'r',\n",
    "            alpha = 0.6,\n",
    "            label = 'Background')\n",
    "\n",
    "_ = ax.hist(signal_ts,\n",
    "            bins = bins,\n",
    "            weights = np.ones(len(signal_ts), dtype=float)/len(signal_ts),\n",
    "            color = 'b',\n",
    "            alpha = 0.6,\n",
    "            label = 'Signal+Background')\n",
    "\n",
    "# Let's also directly show the location of the median \n",
    "# of the background distribution\n",
    "background_median = np.percentile(background_ts, 99.73)\n",
    "ax.axvline(background_median,\n",
    "           linewidth=3,\n",
    "           color = 'k',\n",
    "           alpha = 0.4,\n",
    "           linestyle = 'dashed',\n",
    "           label = r\"Background 5$\\sigma$\")\n",
    "\n",
    "# We want 90% of the (signal+background) to be\n",
    "# above the sensitivity, but np.percentile takes\n",
    "# the percentage *below* instead.\n",
    "signal_10pc = np.percentile(signal_ts, 50)\n",
    "ax.axvline(signal_10pc,\n",
    "           linewidth=3,\n",
    "           color = 'k',\n",
    "           alpha = 0.4,\n",
    "           linestyle = 'dotted',\n",
    "           label = \"Signal 50%\")\n",
    "\n",
    "\n",
    "ax.legend(fontsize=16)\n",
    "ax.grid(alpha=0.2)\n",
    "\n",
    "ax.set_xlim(bins.min(), bins.max())\n",
    "ax.set_xlabel(\"Test Statistic\", fontsize=16)\n",
    "ax.set_ylabel(\"Number of trials\", fontsize=16,)\n",
    "ax.tick_params(which=\"both\", labelsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
